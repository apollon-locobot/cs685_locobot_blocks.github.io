<!DOCTYPE html>
<html>

<head>
  <title>CS685 Final Project Tutorial</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;400;500;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0" />
  <script src="script.js" defer></script>
</head>

<body>
  <header class="bgimg w3-display-container" id="home">
    <div class="w3-display-right stroke">
      <span class="w3-text-white" style="font-size:35px">Real Robotics Planning <br>and Control with LoCoBot</span>
    </div>
  </header>

  <div id="mySidenav" class="sidenav">
    <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
    <a href="#introduction">INTRODUCTION</a>
    <a href="#locobot">LOCOBOT PX100</a>
    <a href="#quick-guide">QUICK GUIDE</a>
    <a href="#objective">OBJECTIVE</a>
    <a href="#apriltag">APRILTAG</a>
    <a href="#bearing-range">BEARING & RANGE</a>
    <a href="#kinematics">SEARCH & IK</a>
    <a href="#landmark-slam">LANDMARK SLAM</a>
    <a href="#pid">PID CONTROL</a>
    <a href="#rosbag">ROSBAG</a>
    <a href="#demo">DEMO</a>
    <a href="#conclusion">CONCLUSION</a>
    <a href="#extensions">FUN EXTENSIONS</a>
    <a href="#tips">SIMPLIFICATIONS & TIPS</a>
    <a href="#credits">CREDITS</a>
    <a href="#references">REFERENCES</a>
  </div>

  <span id="menu-icon" onclick="openNav()" class="material-symbols-outlined menu-icon">menu</span>

  <div id="main" onclick="closeNav()">
    <div class="w3-sand w3-large">
      <div id="introduction">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">INTRODUCTION</span></h5>
        <p class="content">This project was done as a part of the CS685 Autonomous Robotics course at George Mason University. We were interested in experimenting with a real robot and chose to work on the <a href="https://www.trossenrobotics.com/locobot-px100.aspx" target="_blank">LoCoBot PX100</a>, since the Interbotix APIs make it easier to work with. We chose a "toy problem" (ðŸ™‚) that incorporates the concepts of Inverse Kinematics, Control, Probabilistic Robotics, Tracking and Localization, SLAM that were taught in the <a href="https://cs.gmu.edu/media/syllabi/Fall2022/CS_685SteinG001.pdf" target="_blank">course</a>, to let a robot navigate through and interact with the environment to perform basic tasks. The project was developed using <a href="http://wiki.ros.org/noetic" target="_blank">ROS Noetic</a>, which the LoCoBot supports seamlessly.</p>

        <p class="content next">
          This tutorial is an informal introduction to using the LoCoBot and a walkthrough of each of the components that make up our project. We'll also discuss some practical tips and the challenges we faced. The LoCoBots offer a variety of sensors and capabilities that are very accessible to beginners, students, and researchers. We only scratched the surface of what is possible. We hope that sharing our experience through this tutorial will assist future students in leveraging the GMU Autonomous Robotics Lab's fleet of LoCoBots. We look forward to the cool projects to come!
        </p>

        <center><img src="/assets/images/locobots.jpg" style="width:20%;" class="w3-margin-top"></center>

        <p class="content">
          <span class="material-symbols-outlined" style="color: #FF7900;vertical-align: text-bottom;">warning</span> Warning: You will have a ton of fun working with a real robot. It will also take a lot more time than you think and be full of unexpected challenges. Be prepared for some long days and late nights! We highly recommend breaking your project down into distinct subgoals and make incremental progress. Be ambitious, but have a plan and get started right away.
        </p>
      </div>

      <div id="locobot">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">LOCOBOT PX100</span></h5>
        <div class="row">
          <div class="column">
            <p class="content">
              LoCoBot PX100, a.k.a "Low-Cost Robot", from Trossen Robotics starts from about $3000 and has the following features:
            <ul>
              <li>Create 3 from iRobot (base)</li>
              <li>Intel NUC NUC8i3BEH Mini PC</li>
              <li>IntelÂ® RealSenseâ„¢ Depth Camera D435</li>
              <li>PincherX 100 Robot Arm (4 degrees of freedom)</li>
              <li>RPLIDAR A2M8 360Â° Laser Range Scanner</li>
              <li>MoveIt Support</li>
            </ul>
            </p>
          </div>
          <div class="column">
            <center><img src="/assets/images/px100_other_side.jpg" style="width:40%;" class="w3-margin-top"></center>
          </div>
        </div>
      </div>

      <div id="quick-guide">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">QUICK GUIDE</span></h5>
        <p class="content">
          <!-- TODO: Sai, Aaron -->
          <i>Add a quick guide on how to use LoCoBot and anything to work on this project. Pointers:</i>
        <ul>
          <li>turning the locobot on/off</li>
          <li>sa, sam, saj</li>
          <li>The local website</li>
          <li>arm, camera, base, joints, etc of the bot</li>
          <li>python scipt</li>
          <li>Nav Stack - https://www.trossenrobotics.com/docs/interbotix_xslocobots/ros_packages/navigation_stack_configuration.html (LoCoBot needs to be connected to a monitor for this)</li>
        </ul>
        </p>
      </div>

      <div id="objective">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">PROJECT OBJECTIVE - TOY COLLECTION</span></h5>
        <p class="content">
          The complete goal of the problem is for the robot to search for a block using its AprilTag, travel to almost near the block using landmark SLAM, pickup the block using Inverse Kinematics, travel back to the origin, and drop the block into the bin.
        </p>
        <center>
          <img src="/assets/images/Project_Flow_Chart.png" style="width:90%;" class="w3-margin-top">
          <div class="img-desc">Fig. 1. The objective of the project</div>
        </center>
      </div>

      <div id="apriltag">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">APRILTAG</span></h5>
        <div class="w3-panel w3-leftbar w3-light-grey" style="margin: 26px;">
          <p>
            <i>
              AprilTag is a visual fiducial system, useful for a wide variety of tasks including augmented reality, robotics, and camera calibration. Targets can be created from an ordinary printer, and the AprilTag detection software computes the precise 3D position, orientation, and identity of the tags relative to the camera. The AprilTag library is implemented in C with no external dependencies. It is designed to be easily included in other applications, as well as be portable to embedded devices. Real-time performance can be achieved even on cell-phone grade processors.
            </i>
          </p>
          <p>
            - <a href="https://april.eecs.umich.edu/software/apriltag" target="_blank">AprilTag</a>
          </p>
        </div>
        <div class="content">
          <p>
            In this project, AprilTags were used to identify the block and the landmarks. This is a useful project simplification to avoid computer vision tasks such as camera depth perception and object detection (which could be a project all on its own), as well as the error those could introduce.
          </p>
          <p>
            The <code>/tag_detections</code> topic provides the list of AprilTag detections. The relevant AprilTag information was extracted from this and stored in the class variables (check <code>get_tag_data()</code>).
          </p>
          <b>Understanding AprilTag values:</b> <br>
          <span style="margin-left: 32px;">
            The raw AprilTag <code>detections</code> provided by <code>/tag_detections</code> topic include the pose and orientation of the AprilTag with respect to the camera (see Fig. 3). To understand the values, let's consider the AprilTag with ID 5 (the tag on the robot arm in Fig. 2). The <code>detections[0].pose.pose.pose</code> has the required information, where <code>position</code> is the estimated position of the AprilTag and the <code>orientation</code> is the tilt of the tag in all dimensions with respect to the camera's field.
          </span>
        </p>
        <div class="row">
          <div class="column">
            <center>
              <img src="/assets/images/tag_detections_image.png" style="width:65%;" class="w3-margin-top">
              <div class="img-desc">Fig. 2. The feed published by <code>/tag_detections_image</code> topic</div>
            </center>
          </div>
          <div class="column">
            <center>
              <img src="/assets/images/tag_detections.png" style="width:35%;" class="w3-margin-top">
              <div class="img-desc">Fig. 3. The detections published by <code>/tag_detections</code> topic</div>
            </center>
          </div>
        </div>
        <p class="content">
          To understand the <code>position</code>, visualize a line <code>L</code> going from the center of the camera going into the 3D field of view. At the <code>position.z (Z)</code> distance from the camera, draw a 2D plane <code>P</code> (that contains the AprilTag), marking its intersection with <code>L</code> as the center of the plane, say <code>O</code>. Now, <code>position.x (X)</code> is the horizontal distance from the origin <code>O</code> (right-negative, left-positive) and <code>position.y (Y)</code> is the vertical distance form the origin <code>O</code> (top-negative, bottom-positive). The projections in Fig. 4, 5, 6 below might be helpful in understanding this.
        </p>
        <div class="row">
          <div class="column">
            <center>
              <img src="/assets/images/apriltag_expl_1.jpg" style="width:90%;" class="w3-margin-top">
              <div class="img-desc">Fig. 4. View of the robot and the AprilTag</div>
            </center>
          </div>
          <div class="column">
            <center>
              <img src="/assets/images/apriltag_expl_2.jpg" style="width:90%;" class="w3-margin-top">
              <div class="img-desc">Fig. 5. Camera's view of the AprilTag</div>
            </center>
          </div>
          <div class="column">
            <center>
              <img src="/assets/images/apriltag_expl_3.jpg" style="width:90%;" class="w3-margin-top">
              <div class="img-desc">Fig. 5. Bird's-eye view of the robot and the AprilTag</div>
            </center>
          </div>
        </div>
        <p class="content">
          The <code>size</code> seen in Fig. 3 (i.e., 0.02) is the size of the AprilTag configured in <code>~/mds-locobot/apriltag-files/tags.yml</code> Any new AprilTags should be added to this file.
        </p>
        <div class="w3-panel w3-leftbar w3-light-grey" style="margin: 26px;">
          <p>
            <i>
              Note: The tag size should not be measured from the outside of the tag. The tag size is defined as the distance between the detection corners, or alternately, the length of the edge between the white border and the black border. The following illustration marks the detection corners with red Xs and the tag size with a red arrow for a tag from the 48h12Custom tag family.
            </i>
          </p>
          <p>
            - <a href="https://github.com/AprilRobotics/apriltag/wiki/AprilTag-User-Guide" target="_blank">AprilTag User Guide</a>
          </p>
        </div>
      </div>

      <div id="bearing-range">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">BEARING & RANGE</span></h5>
        <p class="content">
          By placing AprilTags on objects or on the ground, they can be used to calculate the bearing and range from the robot's center to the center of the AprilTag. These values are always relative to the robot's pose at the time of the detection. We used this in two ways: to estimate the block's coordinates to drive close to it, and to detect relative landmark positions to aide in localization.
        </p>
        <p>
          We use 2D odometry, however the tag's <code>position.z</code> value is the 3D distance to the tag's center. Therefore we need to use the <code>position.y</code> value to project to a line parallel to the camera. Since the camera can be in any tilt position, we need to account for tha
        </p>

        <!-- TODO: Aaron -->
        <ul>
          <li>Talk about shift in camera x and y, with respect to the robot center</li>
        </ul>
      </div>

      <div id="kinematics">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">SEARCH & INVERSE KINEMATICS</span></h5>
        <p class="content">
          The first subgoal was to search the block by rotating and grab it, when the base is stationary. Then the robot was certain of its location when SLAM was implemented, so we integrated motion with PID and started searching for the block outside its view. When initiated, the robot first looks for the block in its current view, then on its left, and then on its right (rotating by <code>ROTATION_INCREMENT</code> w.r.t origin). If the robot doesn't find the block it moves forward by <code>SEARCH_INCREMENT_X</code> meters. If in any of the view the robot finds the block, it travels to a point that is <code>BLOCK_TRAVEL_RADIUS</code> meters away from the block and starts alignment. As the robot searches in one direction, we only check left, straight, and right covering specific field only. The search code is available <code>find_goal()</code>.

          <script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fawpollon%2Fcs685_locobot_blocks%2Fblob%2F5f9e61d8196ca24718a6d3e725c17d75670eaf8c%2Fblockbot.py%23L166-L209&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></script>

        </p>

        <p class="content">
          Since the LoCoBot PX100 has only 4 degrees of freedom, the arm doesn't have a wide field of reach for the end effector. So we chose to align the robot with the block so it always end up in a position that makes it easier to grab the block. For the alignment part in <code>align_with_block()</code>, the block is expected to be near the robot. The robot looks in the nearby field (hence the camera tilt at the beginning), and then slowly rotates or moves forward using PID controllers until the block is in <code>[-ALIGN_BEARING_ACCEPTANCE, ALIGN_BEARING_ACCEPTANCE]</code> and <code>[-ALIGN_RADIUS_ACCEPTANCE, ALIGN_RADIUS_ACCEPTANCE]</code> ranges. As PID controllers are used, the motion is inversely proportional to the distance/angle from the block, which ensures accurate alignment with the block.

          <script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fawpollon%2Fcs685_locobot_blocks%2Fblob%2F5f9e61d8196ca24718a6d3e725c17d75670eaf8c%2Fblockbot.py%23L248-L294&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></script>

          To obtain these fixed distance and angle of alignment, we've experimented with multiple positions of the block and found the one that makes it easy to grab. Once aligned the <code>grab_block()</code> function is used to grab the block. The robot's arm is set to the home pose, and in the <code>set_ee_cartesian_trajectory()</code>, <code>z</code> is set to <code>-0.25</code> which makes the arm reach the ground from the home position. At this point the block would be withing the grippers range but at a little far, to account for any errors in alignment. Then the robot moves forward by <code>GRABBING_ERROR_DISTANCE</code> meters, grabs the block, and returns the arm to the sleep pose. This function internally uses MoveIt to check if a plan exists to reach the specified position and executes if one exists.

          <script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fawpollon%2Fcs685_locobot_blocks%2Fblob%2F977e8ff2cbe6b5e445cd00590cf8ced1bcd3f0a8%2Fblockbot.py%23L145-L156&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></script>
        </p>
      </div>

      <div id="landmark-slam">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">LANDMARK SLAM</span></h5>
        <!-- TODO: Aaron -->
        <ul>
          <li>talk about AprilTags on floor used as landmarks</li>
        </ul>
      </div>

      <div id="pid">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">PID CONTROL</span></h5>
        <!-- TODO: Aaron -->
      </div>

      <div id="rosbag">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">ROSBAG</span></h5>
        <p class="content">
          <!-- TODO: Sashank -->
          <i>Add details on how to use rosbag and why it's useful.</i>
        </p>
      </div>

      <div id="demo">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">DEMO</span></h5>
        <p class="content">Here's a demo of the robot following the <a href="#objective">plan</a>.</p>
        <center>
          <div id="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/rwvMlPIRUuM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </center>
        <p class="content">
          <b>Description:</b> <br>
          <span style="margin-left: 32px;">
            The robot starts at the origin (x=0, y=0, yaw=0) with the bin right behind it (x=0, y=0, yaw=-Ï€). The landmarks (large AprilTags) were placed on the ground, the block was placed within the <code>CAMERA_SETTINGS.search_tilt</code> view. Once initiated, the robot searches for the block infront, left, and right of it. As the block isn't detectedm the robot moves forward by <code>SEARCH_INCREMENT_X</code> meters, and searches in the three fields of view again. Though the block can be seen now, it isn't withing <code>BLOCK_DETECTION_RANGE</code> meters, so it moves forward. Now, as the block is nearby, the robot travels in the direction of the block such that it is <code>BLOCK_TRAVEL_RADIUS</code> meters away from the block. It tilts the camera to <code>CAMERA_SETTINGS.tilt</code> and starts aligning with the block until it is <code>GRABBING_RADIUS</code> away from the block. The arm then goes to the home pose, then to the ground, move forward by <code>GRABBING_ERROR_DISTANCE</code> meters, grabs the block, and returns the arm to the sleep pose. When in motion, the robot detects the landmarks on the ground and uses landmark localization through GTSAM to accurately predict its location. With the block grabbed, the robot travels back to the origin, but to the bin's pose (0, 0, -Ï€). It then moves the arm to the home pose, moves forward, dropts the block, moves the arm to the sleep pose, and rotates back to the initial pose (0, 0, 0).
          </span>
        </p>
      </div>

      <div id="conclusion">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">CONCLUSION</span></h5>
        <!-- TODO: Aaron -->
        <ul>
          <li>challanges in pid, slam, trig</li>
          <li>writing functions for each functionality</li>
        </ul>
      </div>

      <div id="extensions">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">FUN EXTENSIONS</span></h5>
        <!-- TODO: Sai -->
        <p class="content">
          Here are some subgoals we wanted to achieve, but couldn't because of the limited time:
        <ul>
          <li><b>Multi-block search</b> - Perform the same plan, go back and search for another block</li>
          <li><b>Bin search</b> - Just like the block, search for the bin in the environment. The <code>find_goal()</code> is already configured to search for an AprilTag on the bin</li>
          <li><b>Block sorting</b> - Have multiple bins and blocks, place a block in a specific bin</li>
        </ul>
        </p>
      </div>

      <div id="tips">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">SIMPLIFICATIONS & TIPS</span></h5>
        <p class="content">
          Here are some things that made our life easy (ðŸ˜‰):
        <ul>
          <li>To avoid using Deep Learning models (like YOLO) to find the block using vision, we've added an AprilTag to the block</li>
          <li>Since there are two camera tilts to search for the block from far away and align when near the block, two AprilTags were added for accurate detections in both tilts</li>
          <center><img src="/assets/images/block.jpg" style="width:15%;" class="w3-margin-top"></center>
          <li>The bin, in which the block is dropped, is placed right behind the initial pose so the robot doesn't have to search for the bin</li>
          <li>Since the motion after finding the block in search is faster than the alignment motion, the robot travels to a distance away from the block to speed the plan</li>
          <li>Since the AprilTag on the block is small, the robot might not track it all the times. So the search has a threshold that ensures travel doesn't start until the block is within <code>BLOCK_DETECTION_RANGE</code> meters</li>
          <li>The path to the block and the bin is free from obstacles</li>
          <li>The landmark AprilTags were placed on the ground, to make use of the same camera tilt used for search and not switch between tilts. Only camera tilts were used, no pans</li>
          <li>When in motion, a validation is added to check if the base bumper hits anything to stop the motion (no rogue tobots!)</li>
          <li>The topic <code>/scan</code> can be used to get the LIDAR readings</li>
          <li>Setting <a href="https://code.visualstudio.com/docs/remote/ssh" target="_blank">Remote Development using SSH</a> in VS Code could save lots of time</li>
        </ul>
        </p>
      </div>

      <div id="credits">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">CREDITS</span></h5>
        <p class="content">
          We're grateful to the following people in helping us complete this project:
        <ul>
          <li>Michael D Schader, GMU</li>
          <li>Prof. Gregory J. Stein, GMU</li>
          <li>Prof. Sean Luke, GMU</li>
        </ul>
        </p>
      </div>

      <div id="references">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">REFERENCES</span></h5>
        <p class="content">
        <ul>
          <li><a href="http://wiki.ros.org/apriltag_ros" target="_blank">AprilTags</a></li>
          <li><a href="https://www.trossenrobotics.com/docs/interbotix_xsarms/index.html" target="_blank">Interbotix Arms</a></li>
          <li><a href="https://github.com/Interbotix/interbotix_ros_toolboxes/blob/cdb8e7b77eb6be31d20bc77c947f5ba34525f4ec/interbotix_xs_toolbox/interbotix_xs_modules/src/interbotix_xs_modules/locobot.py" target="_blank">Interbotix LoCoBot API</a></li>
          <li><a href="https://pidexplained.com/how-to-tune-a-pid-controller/" target="_blank">How to tune PID Controller</a></li>
          <li><a href="https://gtsam.org/" target="_blank">GTSAM</a></li>
        </ul>
        </p>
      </div>

    </div>

    <footer class="w3-center w3-light-grey w3-padding-48 w3-large">
      <p>A project by <a href="https://www.linkedin.com/in/aaron-pollon-8a8b2246/" target="_blank">Aaron Pollon</a>, <a href="https://www.linkedin.com/in/sai-kishore-salaka-0112358/" target="_blank">Sai Kishore Salaka</a>, and Sai Sashank</p>
      <p style="font-size: 12px; position: absolute; right: 36px;">Website powered by <a href="https://www.w3schools.com/w3css/default.asp" target="_blank">w3.css</a></p>
    </footer>
  </div>
</body>

</html>