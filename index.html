<!DOCTYPE html>
<html>

<head>
  <title>CS685 Final Project Tutorial</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;400;500;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0" />
  <script src="script.js" defer></script>
</head>

<body>
  <header class="bgimg w3-display-container" id="home">
    <div class="w3-display-right stroke">
      <span class="w3-text-white" style="font-size:35px">Real Robotics Planning <br>and Control with LoCoBot</span>
    </div>
  </header>

  <div id="mySidenav" class="sidenav">
    <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
    <a href="#introduction">INTRODUCTION</a>
    <a href="#locobot">LOCOBOT PX100</a>
    <a href="#quick-guide">QUICK GUIDE</a>
    <a href="#objective">OBJECTIVE</a>
    <a href="#apriltag">APRILTAG</a>
    <a href="#bearing-range">BEARING & RANGE</a>
    <a href="#kinematics">IK</a>
    <a href="#landmark-slam">LANDMARK SLAM</a>
    <a href="#pid">PID CONTROL</a>
    <a href="#rosbag">ROSBAG</a>
    <a href="#demo">DEMO</a>
    <a href="#conclusion">CONCLUSION</a>
    <a href="#extensions">FUN EXTENSIONS</a>
    <a href="#tips">SIMPLIFICATIONS & TIPS</a>
    <a href="#credits">CREDITS</a>
    <a href="#references">REFERENCES</a>
  </div>

  <span id="menu-icon" onclick="openNav()" class="material-symbols-outlined menu-icon">menu</span>

  <div id="main">
    <div class="w3-sand w3-large">
      <div id="introduction">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">INTRODUCTION</span></h5>
        <p class="content">This project was done as a part of the CS685 Autonomous Robotics course at George Mason University. We were interested in experimenting with a real robot and chose to work on the <a href="https://www.trossenrobotics.com/locobot-px100.aspx" target="_blank">LoCoBot PX100</a>, since the Interbotix APIs make it easier to work with. We chose a "toy problem" (ðŸ™‚) that incorporates the concepts of Inverse Kinematics, Control, Probabilistic Robotics, Tracking and Localization, SLAM taught in the <a href="https://cs.gmu.edu/media/syllabi/Fall2022/CS_685SteinG001.pdf" target="_blank">course</a>, to let a robot navigate through and interact with the environment to perform basic tasks. The project was developed using <a href="http://wiki.ros.org/noetic" target="_blank">ROS Noetic</a>, which the LoCoBot supports seamlessly.</p>

        <p class="content next">
          This tutorial is an informal introduction to using the LoCoBot and a walkthrough of each of the components that make up our project. We'll also discuss some practical tips and the challenges we faced. The LoCoBots offer a variety of sensors and capabilities that are very accessible to beginners, students, and researchers. We only scratched the surface of what is possible. We hope that sharing our experience through this tutorial will assist future students in leveraging the GMU Autonomous Robotics Lab's fleet of LoCoBots. We look forward to the cool projects to come!
        </p>

        <p class="content">
          <span class="material-symbols-outlined" style="color: #FF7900;vertical-align: text-bottom;">warning</span> Warning: You will have a ton of fun working with a real robot. It will also take a lot more time than you think and be full of unexpected challenges. Be prepared for some long days and late nights! We highly recommend breaking your project down into distinct subgoals and make incremental progress. The sections in this tutorial are the subgoals we achieved in an order to complete this project. Be ambitious, but have a plan and get started right away.
        </p>
      </div>

      <div id="locobot">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">LOCOBOT PX100</span></h5>
        <div class="row">
          <div class="column">
            <p class="content">
              LoCoBot PX100, a.k.a "Low-Cost Robot", from Trossen Robotics starts from about $3000 and has the following features:
            <ul>
              <li>Create 3 from iRobot (base)</li>
              <li>Intel NUC NUC8i3BEH Mini PC</li>
              <li>IntelÂ® RealSenseâ„¢ Depth Camera D435</li>
              <li>PincherX 100 Robot Arm (4 degrees of freedom)</li>
              <li>RPLIDAR A2M8 360Â° Laser Range Scanner</li>
              <li>MoveIt Support</li>
            </ul>
            </p>
          </div>
          <div class="column">
            <center><img src="/assets/images/px100_other_side.jpg" style="width:40%;" class="w3-margin-top"></center>
          </div>
        </div>
      </div>

      <div id="quick-guide">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">QUICK GUIDE</span></h5>
        <p class="content">
          <!-- TODO: Sai, Aaron -->
          <i>Add a quick guide on how to use LoCoBot and anything to work on this project. Pointers:</i>
        <ul>
          <li>turning the locobot on/off</li>
          <li>sa, sam, saj</li>
          <li>The local website</li>
          <li>arm, camera, base, joints, etc of the bot</li>
          <li>python scipt</li>
          <li>Nav Stack - https://www.trossenrobotics.com/docs/interbotix_xslocobots/ros_packages/navigation_stack_configuration.html</li>
        </ul>
        </p>
      </div>

      <div id="objective">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">PROJECT OBJECTIVE - TOY COLLECTION</span></h5>
        <p class="content">
          The complete goal of the problem is for the robot to search for a block using its AprilTag, travel to almost near the block using landmark SLAM, pickup the block using Inverse Kinematics, travel back to the origin, and drop the block into the bin.
        </p>
        <center>
          <img src="/assets/images/Project_Flow_Chart.png" style="width:90%;" class="w3-margin-top">
          <div class="img-desc">Fig. 1. The objective of the project</div>
        </center>
      </div>

      <div id="apriltag">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">APRILTAG</span></h5>
        <div class="w3-panel w3-leftbar w3-light-grey" style="margin: 26px;">
          <p>
            <i>
              AprilTag is a visual fiducial system, useful for a wide variety of tasks including augmented reality, robotics, and camera calibration. Targets can be created from an ordinary printer, and the AprilTag detection software computes the precise 3D position, orientation, and identity of the tags relative to the camera. The AprilTag library is implemented in C with no external dependencies. It is designed to be easily included in other applications, as well as be portable to embedded devices. Real-time performance can be achieved even on cell-phone grade processors.
            </i>
          </p>
          <p>
            - <a href="https://april.eecs.umich.edu/software/apriltag" target="_blank">AprilTag</a>
          </p>
        </div>
        <p class="content">
          In this project, AprilTags were used to identify the block and the landmarks. The <code>/tag_detections</code> topic provides the list of AprilTag detections. The relevant AprilTags are extracted from this and stored in the class variables (check <code>get_tag_data()</code>).
        </p>
        <p class="content">
          <b>Understanding AprilTag values:</b> <br>
          <span style="margin-left: 32px;">
            The raw AprilTag <code>detections</code> provided by <code>/tag_detections</code> topic include the pose and orientation of the AprilTag with respect to the camera (see Fig. 3). To understand the values, let's consider the AprilTag with ID 5 (the tag on the robot arm in Fig. 2). The <code>detections[0].pose.pose.pose</code> has the required information, where <code>position</code> is the estimated position of the AprilTag and the <code>orientation</code> is the tilt of the tag in all dimensions with respect to the camera's field.
          </span>
        </p>
        <div class="row">
          <div class="column">
            <center>
              <img src="/assets/images/tag_detections_image.png" style="width:65%;" class="w3-margin-top">
              <div class="img-desc">Fig. 2. The feed published by <code>/tag_detections_image</code> topic</div>
            </center>
          </div>
          <div class="column">
            <center>
              <img src="/assets/images/tag_detections.png" style="width:35%;" class="w3-margin-top">
              <div class="img-desc">Fig. 3. The detections published by <code>/tag_detections</code> topic</div>
            </center>
          </div>
        </div>
        <p class="content">
          To understand the <code>position</code>, visualize a line <code>L</code> going from the center of the camera going into the 3D field of view. At the <code>position.z (Z)</code> distance from the camera, draw a 2D plane <code>P</code>, marking its intersection with <code>L</code> as the center of the plane, say <code>O</code>. From this point, <code>position.x (X)</code> is the horizontal distance from the origin <code>O</code> (right-negative, left-positive) and <code>position.y (Y)</code> is the vertical distance form the origin <code>O</code> (top-negative, bottom-positive). The projections in Fig. 4, 5, 6 might be helpful in understanding this.
        </p>
        <div class="row">
          <div class="column">
            <center>
              <img src="/assets/images/apriltag_expl_1.jpg" style="width:90%;" class="w3-margin-top">
              <div class="img-desc">Fig. 4. View of the robot and the AprilTag</div>
            </center>
          </div>
          <div class="column">
            <center>
              <img src="/assets/images/apriltag_expl_2.jpg" style="width:90%;" class="w3-margin-top">
              <div class="img-desc">Fig. 5. Camera's view of the AprilTag</div>
            </center>
          </div>
          <div class="column">
            <center>
              <img src="/assets/images/apriltag_expl_3.jpg" style="width:90%;" class="w3-margin-top">
              <div class="img-desc">Fig. 5. Bird's-eye view of the robot and the AprilTag</div>
            </center>
          </div>
        </div>
        <p class="content">
          The <code>size</code> seen in Fig. 3 (i.e., 0.02) is the size of the AprilTag configured in <code>~/mds-locobot/apriltag-files/tags.yml</code>.
        </p>
        <div class="w3-panel w3-leftbar w3-light-grey" style="margin: 26px;">
          <p>
            <i>
              Note: The tag size should not be measured from the outside of the tag. The tag size is defined as the distance between the detection corners, or alternately, the length of the edge between the white border and the black border. The following illustration marks the detection corners with red Xs and the tag size with a red arrow for a tag from the 48h12Custom tag family.
            </i>
          </p>
          <p>
            - <a href="https://github.com/AprilRobotics/apriltag/wiki/AprilTag-User-Guide" target="_blank">AprilTag User Guide</a>
          </p>
        </div>
      </div>

      <div id="bearing-range">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">BEARING & RANGE</span></h5>
        <!-- TODO: Aaron -->
        <ul>
          <li>Talk about shift in camera x and y, with respect to the robot center</li>
        </ul>
      </div>

      <div id="kinematics">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">INVERSE KINEMATICS</span></h5>
        <p class="content">
          The first subgoal was to get the block grabbing work. Since the LoCoBot PX100 has only 4 degrees of freedom, the arm didn't have much field of reach for the end effector. So we chose to align the robot with the block so it always end up in a position which makes it easier to grab.
        </p>
        <p class="content">
          For the alignment part in <code>align_with_block()</code>, the block is expected to be near the robot. The robot looks in the nearby field (hence the camera tilt at the beginning), and then slowly rotates or moves forward to align with the block. The slow motion ensures proper position and accurate April Tag readings. The alignmnet stops once the robot is at a specific distance and angle with respect to the block.

          <script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fawpollon%2Fcs685_locobot_blocks%2Fblob%2F8f35c934f755259fcaecc0463301a5b61adc14c7%2Fblockbot.py%23L205-L243&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></script>

          To obtain these fixed distance and angle of alignment, we've experimented with multiple positions of the block and found the one that makes it easy to grab. Once aligned the <code>grab_block()</code> function is used to grab the block. The robot's arm is set to the home pose, and in the <code>set_ee_cartesian_trajectory()</code>, <code>z</code> is set to <code>-0.25</code> which makes the arm reach the ground from the home position, where the block would be. This function internally uses MoveIt to check if a plan exists to reach the specified position and executes if one exists.

          <script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fawpollon%2Fcs685_locobot_blocks%2Fblob%2F8f35c934f755259fcaecc0463301a5b61adc14c7%2Fblockbot.py%23L140-L148&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></script>

          The bin was placed just behind the initial pose of the robot, so dropping the block is simple, the robot
        </p>
      </div>

      <div id="landmark-slam">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">LANDMARK SLAM</span></h5>
        <!-- TODO: Aaron -->
        <ul>
          <li>talk about AprilTags on floor used as landmarks</li>
        </ul>
      </div>

      <div id="pid">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">PID CONTROL</span></h5>
        <!-- TODO: Aaron -->
      </div>

      <div id="rosbag">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">ROSBAG</span></h5>
        <p class="content">
          <!-- TODO: Sashank -->
          <i>Add details on how to use rosbag and why it's useful.</i>
        </p>
      </div>

      <div id="demo">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">DEMO</span></h5>
        <p class="content">Here's a demo of the robot following the <a href="#objective">plan</a>.</p>
        <center>
          <div id="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/OkzbEALzFM8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </center>
        <p class="content">
          <b>Description:</b> <br>
          <span style="margin-left: 32px;">
            The robot starts at the origin (x=0, y=0, yaw=0) with the bin right behind it (x=0, y=0, yaw=-Ï€). The landmarks (large AprilTags) were placed on the ground, the block was placed withing the camera's search tilt view. Once initiated, the robot finds the block, calculates the bearing & range, moves forward until it is 35 cms away from the block. Then, it tilts the camera to aligning angle and slowly aligns itself until the block is straight ahead of the gripper (angle=0) and the block is 32.5 cms away from the robot. The arm then goes to the home pose, then moves to the ground, grabs the block, and moves the arm to sleep pose. In the motion towards the block, the landmark localization using GTSAM keeps running to accurately estimate the position of the robot. It uses this information to travel back to the origin, but to the bin's pose (0, 0, -Ï€). Now the robot just needs to move the arm to the home pose, move forward, drop the block, move back, move the arm to sleep pose, and rotates back to the initial pose (0, 0, 0).
          </span>
        </p>
      </div>

      <div id="conclusion">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">CONCLUSION</span></h5>
        <!-- TODO: Aaron -->
        <ul>
          <li>challanges in pid, slam, trig</li>
          <li>writing functions for each functionality</li>
        </ul>
      </div>

      <div id="extensions">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">FUN EXTENSIONS</span></h5>
        <!-- TODO: Sai -->
        <p class="content">
          Here are some subgoals we wanted to achieve, but couldn't because of the limited time:
        <ul>
          <li><b>Multi-block search</b> - Perform the same plan, go back and search for another block</li>
          <li><b>Handle erroneous block pickup</b> - If the block wasn't picked up (due to an error in range estimation), which can be identified from the gripper joint value, move back and align with the block again</li>
          <li><b>Bin search</b> - Just like the block, search for the bin in the environment</li>
          <li><b>Block sorting</b> - Have multiple bins and blocks, place a block in a specific bin</li>
        </ul>
        </p>
      </div>

      <div id="tips">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">SIMPLIFICATIONS & TIPS</span></h5>
        <p class="content">
          Here are some things that made our life easy (ðŸ˜‰):
        <ul>
          <li>To avoid using Deep Learning models, like YOLO, to find the block using vision, we've added an AprilTag to the block</li>
          <li>Since there are two camera angles to search for the block from far away and then align when near the block, two AprilTags were added for accurate detections in both tilts</li>
          <li>The bin, in which the block is dropped, is placed right behind the initial pose so the robot doesn't have to search for the bin</li>
          <li>Since the motion after finding the block in search is faster than the alignment motion, the robot travels to exactly 35 cms away from the block (the robot can grab the block when it's 32.5 cms away)</li>
          <li>The path to the block and the bin is free from obstacles</li>
          <li>The landmark AprilTags were placed on the ground, to make use of the same camera tilt used for search and not switch between tilts. Only camera tilts were used, no pans</li>
          <li>When in motion, a validation is added to check if the base bumper hits anything to stop the motion (no rogue tobots!)</li>
          <li>The topic <code>/scan</code> can be used to get the LIDAR readings</li>
          <li>Setting <a href="https://code.visualstudio.com/docs/remote/ssh" target="_blank">Remote Development using SSH</a> in VS Code could save lots of time</li>
        </ul>
        </p>
      </div>

      <div id="credits">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">CREDITS</span></h5>
        <p class="content">
          We're grateful to the following people in helping us complete this project:
        <ul>
          <li>Michael D Schader, GMU</li>
          <li>Prof. Gregory J. Stein, GMU</li>
          <li>Prof. Sean Luke, GMU</li>
        </ul>
        </p>
      </div>

      <div id="references">
        <h5 class="w3-center w3-padding-32"><span class="w3-tag w3-wide">REFERENCES</span></h5>
        <p class="content">
        <ul>
          <li><a href="http://wiki.ros.org/apriltag_ros" target="_blank">AprilTags</a></li>
          <li><a href="https://www.trossenrobotics.com/docs/interbotix_xsarms/index.html" target="_blank">Interbotix Arms</a></li>
          <li><a href="https://github.com/Interbotix/interbotix_ros_toolboxes/blob/cdb8e7b77eb6be31d20bc77c947f5ba34525f4ec/interbotix_xs_toolbox/interbotix_xs_modules/src/interbotix_xs_modules/locobot.py" target="_blank">Interbotix LoCoBot API</a></li>
          <li><a href="https://pidexplained.com/how-to-tune-a-pid-controller/" target="_blank">How to tune PID Controller</a></li>
          <li><a href="https://gtsam.org/" target="_blank">GTSAM</a></li>
        </ul>
        </p>
      </div>

    </div>

    <footer class="w3-center w3-light-grey w3-padding-48 w3-large">
      <p>A project by <a href="https://www.linkedin.com/in/aaron-pollon-8a8b2246/" target="_blank">Aaron Pollon</a>, <a href="https://www.linkedin.com/in/sai-kishore-salaka-0112358/" target="_blank">Sai Kishore Salaka</a>, and Sai Sashank</p>
      <p style="font-size: 12px; position: absolute; right: 36px;">Website powered by <a href="https://www.w3schools.com/w3css/default.asp" target="_blank">w3.css</a></p>
    </footer>
  </div>
</body>

</html>